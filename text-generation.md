# 文本生成


## 项目

| 项目 | 语言 | 预训练模型 | 简介 |
| ---- | ---- | ---- | ---- |
| [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) | 中文 | √ | 中文的GPT2训练代码，使用BERT的Tokenizer或Sentencepiece的BPE model（感谢kangzhonghua的贡献，实现BPE模式需要略微修改train.py的代码）。可以写诗，新闻，小说，或是训练通用语言模型。支持字为单位或是分词模式或是BPE模式（需要略微修改train.py的代码）。支持大语料训练。 |
| [gpt2-ml](https://github.com/imcaspar/gpt2-ml) | 中文 | √ | GPT2 模型， 多语言支持 |
| [roberta_zh](https://github.com/brightmart/roberta_zh) | 中文 | √ | RoBERTa是BERT的改进版，通过改进训练任务和数据生成方式、训练更久、使用更大批次、使用更多数据等获得了State of The Art的效果；可以用Bert直接加载。|
| [Chinese-PreTrained-XLNet](https://github.com/ymcui/Chinese-PreTrained-XLNet) | 中文 | √ |  本项目提供了面向中文的XLNet预训练模型，旨在丰富中文自然语言处理资源，提供多元化的中文预训练模型选择。 我们欢迎各位专家学者下载使用，并共同促进和发展中文资源建设。|
| [fastText](https://github.com/facebookresearch/fastText) | All | √ | fastText is a library for efficient learning of word representations and sentence classification. |


## 文章
